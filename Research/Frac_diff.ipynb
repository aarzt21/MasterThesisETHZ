{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handy-explosion",
   "metadata": {},
   "source": [
    "Alex - Research - 12. 3. 21\n",
    "\n",
    "### Fractionally differentiated features\n",
    "\n",
    "\n",
    "Inferential analysis of data comprises of using a sample of data to describe the characteristics of a population (such as average height or mean return). For such an analysis and inference to be accurate, it is necessary that the underlying data generation process to remain constant. In the context of finance, the mean return and variance of those returns should not change over time. If they change then it is hard to predict the expected return (or risk as defined by the volatility) of that stock in some time in the future.\n",
    "\n",
    "A similar requirement exists in the case of supervised machine learning (SML). In SML, unseen observations are mapped to a set of labeled examples (features) to know the label (decision or outcome) of the new observation. If the data (features, in the case of SML) is not “stationary” or constant then the machine learning algorithm would not be able to correctly infer the label of the new observation. Hence, stationarity becomes a necessary condition for inferential analysis and supervised machine learning. However, asset prices experience trends (or drifts) that make the time series non-stationary. But the underlying trend is also useful in prediction. This leads to a challenge – how can one make the time series stationary while retaining its predictive power (or memory).\n",
    "\n",
    "Hosking [1981] showed that “fractionally differenced processes exhibit long-term persistence and anti-persistence; the dependence between observations a long time span apart decays much more slowly with time span than is the case with commonly used time series models”. Hence, fractional differentiation is used to make the time series stationary while retaining as much memory as possible.\n",
    "\n",
    "The following notebook answer the questions at the back of chapter 5 and in the process explore the concept of fractional differentiation (frac-diff) in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-participant",
   "metadata": {},
   "source": [
    "### Hudson & Thames Article\n",
    "\n",
    "\n",
    "Inferential analysis of data comprises of using a sample of data to describe the characteristics such as mean and standard deviation of a feature of a population. Consider studying heights of men and women in North America or stock prices. For such an analysis and inference to be accurate, it is necessary that the underlying data generation process to remain constant. In the context of finance, the mean return and variance of those returns should be time-invariant (or not change with time). If the underlying process changes as a result of shift in regime, it would be hard to predict expected risk and return of that stock for a future date. \n",
    "\n",
    "A similar requirement exists in the case of supervised machine learning (SML). SML is a process of learning a function that maps an input to an output based on known input-output examples. In this learning process, each example is a pair consisting of an input object (often a vector or features) and an output (or a signal). The supervised learning algorithm analyzes the training examples and infers a transformation function that can be used to map new (unseen) inputs. If the data (features, in the case of SML) are not “stationary” (in other words, their underlying data generation process changes its characteristics) then the machine learning algorithm would not be able to correctly infer the label of the new observation. \n",
    "\n",
    "Therefore, stationarity becomes a necessary condition for inferential analysis and supervised machine learning. But there is a problem here – even though making a series stationary makes inference analysis and SML easier, the series loses its memory (it probably had a trend and that trend is stripped away in the process of integer differencing). This memory is helpful in predicting where will the asset price series be next point in time. This leads to a challenge – how can one make the time series stationary while retaining its predictive power (or memory).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-sucking",
   "metadata": {},
   "source": [
    "Important: \n",
    "\n",
    "- features don't have to be frac diff'd for RF since RF is not a TS model and hence can't understand temporal data. RF and NN assume you data is iid and not a time series (this is why sample weights matter for them). Frac diff shows good results with time series models. \n",
    "- so frac diff seems to be more appropriate for time series models than for RF. \n",
    "- however, what is important for RF is sample weighting if labels are concurrent (which is not the case for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-preservation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
