{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-tractor",
   "metadata": {},
   "source": [
    "Alex 13. 3. 21\n",
    "\n",
    "### Cross validation in finance\n",
    "\n",
    "Cross validation is the gold standard technique used to \n",
    "\n",
    "- estimate the exptected test MSE, i.e. the generalization error of the model (model assessment)\n",
    "- do hyperparameter tuning (model selection)\n",
    "- do both model selection and model assessment via nested CV\n",
    "\n",
    "\n",
    "**The main reason why CV fails in finance is because observations cannot be assumed to be drawn from an IID process, which creates leakage. Concretely, information from the testing set leaks into the training set, which inflates the testing accuracy**\n",
    "\n",
    "(another reason is because the testing set is used multiple times which leads to a multiple testing and selection bias but this \n",
    "is not the focus of this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-aspect",
   "metadata": {},
   "source": [
    "#### Leakage\n",
    "\n",
    "Leakage takes place when the training set contains information that also appears in the testing set. Consider a serially correlated feature $X$ that is associated with labels $Y$ that are formed on **overlapping** data (i.e. $Y_t$ and $Y_{t+1}$ depend on common returns, i.e. common price data):\n",
    "\n",
    "- because of serial correlation $X_t \\approx X_{t+1}$\n",
    "- because labels are derived from overlapping data points $Y_t \\approx Y_{t+1}$\n",
    "- thus $(Y_t, X_t) \\approx (Y_{t+1}, X_{t+1})$. So if a classifier is trained on $(Y_t, X_t)$ it can predict $Y_{t+1}$ based on $X_{t+1}$ pretty well because they are built on same information, i.e. they are dependent. \n",
    "\n",
    "Thus by placing $t$ in training set and $t+1$ in test set, **information leaks from the test set into the training set**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-celtic",
   "metadata": {},
   "source": [
    "Consequence: **When a classifier is trained on $(Y_t, X_t)$ and then is asked to predict $\\text{E}[Y_{t+1} |X_{t+1}]$ the classifier is more likely to achieve $Y_{t+1} = \\text{E}[Y_{t+1} |X_{t+1}]$ even if $X$ is an irrelevant feature. If X is a predictive feature, leakage will enhance the performance of an already valuable strategy.** So in any case, leakage inflates the expected performance of the model. \n",
    "\n",
    "\n",
    "Note: **For leakage to take place, it must occur that $(ùëã_{t}, ùëå_{t}) \\approx (ùëã_{t+1}, ùëå_{t+1})$ , and it does  not suffice that $ùëã_t ‚âà X_{t+1} $ or even $ùëå_{t} ‚âà ùëå_{t+1}$.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-fortune",
   "metadata": {},
   "source": [
    "#### Purged CV \n",
    "\n",
    "\n",
    "Purged CV solves the leakage problem by removing training set observations whose labels overlap with test set labels: if a test set label $y_j$ depends on information $\\Phi_j$, training set labels that depend on $\\Phi_j$ should be removed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proud-investment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"CV.PNG\" width=400 height=240 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"CV.PNG\" width=400 height=240 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-courage",
   "metadata": {},
   "source": [
    "Additionally, one can do an embargo, which removes a number of training observations from the *after* every test set. This prevents further leakage where purging is not enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stupid-bleeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"CV2.PNG\" width=400 height=240 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"CV2.PNG\" width=400 height=240 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-orbit",
   "metadata": {},
   "source": [
    "#### AFML excercises Ch. 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-juice",
   "metadata": {},
   "source": [
    "##### Why is shuffling a dataset before conducting k-fold CV generally a bad idea in finance?\n",
    "\n",
    "Usually in finance we are working with time series structured data, so by shuffling before conducting a k-fold we are sure to have data in our training set that shares information with the data in our testing set. This can easily lead to leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-hudson",
   "metadata": {},
   "source": [
    "##### Why does shuffling defeat the purpose of k-fold in financial datasets?\n",
    "\n",
    "\n",
    "The goal of any cross-validation technique is to estimate the generalization error of a model on an test set which is **independent** of the training set. \n",
    "\n",
    "By shuffling first, we are making sure that the data set will not be independent of the train data because of leakage between the test and train set after the shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-frost",
   "metadata": {},
   "source": [
    "#### Take a pair of matrices (X,y) representing observed features and labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "complete-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlfinlab as ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mlfinlab.cross_validation import ml_cross_val_score\n",
    "from mlfinlab.cross_validation import PurgedKFold\n",
    "from mlfinlab.util.multiprocess import mp_pandas_obj\n",
    "from mlfinlab.sampling import concurrent\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "FNM = 'results_3-5.csv'\n",
    "rf_cfg = {\n",
    "    \"criterion\": 'entropy',\n",
    "    \"max_depth\": 5,\n",
    "    \"n_estimators\": 100,\n",
    "    \"class_weight\": 'balanced_subsample'\n",
    "}\n",
    "\n",
    "\n",
    "X = pd.read_csv(FNM, index_col=0)\n",
    "\n",
    "y = X.pop('bin')\n",
    "weights= X.pop('weights')\n",
    "samples_info_sets = X.pop('t1')\n",
    "\n",
    "idx_shuffle = np.random.permutation(X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjusted-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsi</th>\n",
       "      <th>side</th>\n",
       "      <th>log_ret</th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>mom4</th>\n",
       "      <th>mom5</th>\n",
       "      <th>volatility</th>\n",
       "      <th>autocorr_1</th>\n",
       "      <th>autocorr_2</th>\n",
       "      <th>autocorr_3</th>\n",
       "      <th>autocorr_4</th>\n",
       "      <th>autocorr_5</th>\n",
       "      <th>log_t1</th>\n",
       "      <th>log_t2</th>\n",
       "      <th>log_t3</th>\n",
       "      <th>log_t4</th>\n",
       "      <th>log_t5</th>\n",
       "      <th>sma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-06 16:23:23.744</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-0.001620</td>\n",
       "      <td>-0.001868</td>\n",
       "      <td>-0.003854</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>-0.065977</td>\n",
       "      <td>-0.043867</td>\n",
       "      <td>-0.066452</td>\n",
       "      <td>0.117382</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>-0.000869</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06 16:35:18.127</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>-0.002493</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>-0.003114</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.038315</td>\n",
       "      <td>-0.030594</td>\n",
       "      <td>-0.060636</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>-0.046009</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06 16:40:02.336</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>-0.001747</td>\n",
       "      <td>-0.002991</td>\n",
       "      <td>-0.003364</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>-0.033713</td>\n",
       "      <td>-0.030061</td>\n",
       "      <td>-0.051160</td>\n",
       "      <td>0.170552</td>\n",
       "      <td>-0.044667</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06 16:44:48.791</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.001749</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-0.002995</td>\n",
       "      <td>-0.004238</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>-0.027737</td>\n",
       "      <td>-0.008073</td>\n",
       "      <td>-0.051415</td>\n",
       "      <td>0.187493</td>\n",
       "      <td>-0.041416</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06 17:09:12.134</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>-0.002503</td>\n",
       "      <td>-0.003750</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>-0.003244</td>\n",
       "      <td>-0.065480</td>\n",
       "      <td>-0.000985</td>\n",
       "      <td>0.208039</td>\n",
       "      <td>-0.032036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          rsi  side   log_ret      mom1      mom2      mom3  \\\n",
       "2015-01-06 16:23:23.744  24.0   1.0 -0.001247 -0.001246 -0.001620 -0.001868   \n",
       "2015-01-06 16:35:18.127  20.0   1.0 -0.000999 -0.000999 -0.001248 -0.002493   \n",
       "2015-01-06 16:40:02.336  18.0   1.0 -0.000500 -0.000500 -0.001498 -0.001747   \n",
       "2015-01-06 16:44:48.791  15.0   1.0 -0.001251 -0.001250 -0.001749 -0.002746   \n",
       "2015-01-06 17:09:12.134  12.0   1.0 -0.002131 -0.002129 -0.002129 -0.001253   \n",
       "\n",
       "                             mom4      mom5  volatility  autocorr_1  \\\n",
       "2015-01-06 16:23:23.744 -0.003854 -0.004349    0.001183   -0.065977   \n",
       "2015-01-06 16:35:18.127 -0.002866 -0.003114    0.001158   -0.038315   \n",
       "2015-01-06 16:40:02.336 -0.002991 -0.003364    0.001159   -0.033713   \n",
       "2015-01-06 16:44:48.791 -0.002995 -0.004238    0.001169   -0.027737   \n",
       "2015-01-06 17:09:12.134 -0.002503 -0.003750    0.001180   -0.003244   \n",
       "\n",
       "                         autocorr_2  autocorr_3  autocorr_4  autocorr_5  \\\n",
       "2015-01-06 16:23:23.744   -0.043867   -0.066452    0.117382   -0.012400   \n",
       "2015-01-06 16:35:18.127   -0.030594   -0.060636    0.170363   -0.046009   \n",
       "2015-01-06 16:40:02.336   -0.030061   -0.051160    0.170552   -0.044667   \n",
       "2015-01-06 16:44:48.791   -0.008073   -0.051415    0.187493   -0.041416   \n",
       "2015-01-06 17:09:12.134   -0.065480   -0.000985    0.208039   -0.032036   \n",
       "\n",
       "                           log_t1    log_t2    log_t3    log_t4    log_t5  sma  \n",
       "2015-01-06 16:23:23.744 -0.000374 -0.000249 -0.001991 -0.000497 -0.000869 -1.0  \n",
       "2015-01-06 16:35:18.127 -0.000250 -0.001247 -0.000374 -0.000249 -0.001991 -1.0  \n",
       "2015-01-06 16:40:02.336 -0.000999 -0.000250 -0.001247 -0.000374 -0.000249 -1.0  \n",
       "2015-01-06 16:44:48.791 -0.000500 -0.000999 -0.000250 -0.001247 -0.000374 -1.0  \n",
       "2015-01-06 17:09:12.134  0.000000  0.000877 -0.001253 -0.001251 -0.000500 -1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2015-01-06 16:23:23.744    0\n",
       "2015-01-06 16:35:18.127    1\n",
       "2015-01-06 16:40:02.336    1\n",
       "2015-01-06 16:44:48.791    1\n",
       "2015-01-06 17:09:12.134    1\n",
       "Name: bin, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-applicant",
   "metadata": {},
   "source": [
    "a) Derive the performance from a 10-fold CV of a RF classifier on (X,y) without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "reflected-stability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5096065766294775"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**rf_cfg)\n",
    "\n",
    "cv_spec = KFold(n_splits=10, shuffle=False)\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                        X=X,\n",
    "                        y=y,\n",
    "                        cv=cv_spec)\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-exhaust",
   "metadata": {},
   "source": [
    "b) Now do the same with shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "iraqi-phone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6133235466823252"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**rf_cfg)\n",
    "\n",
    "cv_spec = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                        X=X,\n",
    "                        y=y,\n",
    "                        cv=cv_spec)\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-romania",
   "metadata": {},
   "source": [
    "**Important: The reason why the test MSE of CV with shuffling is higher is because of leakage!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-permit",
   "metadata": {},
   "source": [
    "In a time series context, we are using information from the past to predict the future. By shuffling, we destroy the temporal structure. The train data set will have information relevant for the test data set, it will be able to peak into the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-vatican",
   "metadata": {},
   "source": [
    "#### Now do purged CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "whole-director",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011215502055196"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_gen = PurgedKFold(n_splits=10, samples_info_sets = samples_info_sets, pct_embargo=0.01)\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                        X=X,\n",
    "                        y=y,\n",
    "                        cv=cv_gen)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-zambia",
   "metadata": {},
   "source": [
    "Performance is lower because we prevent leakage, i.e. we remove training samples that share common information with test samples. Thus no information from the test set leaks into the training set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
